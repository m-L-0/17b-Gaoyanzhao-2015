{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "#创建一个reader来读取TFRecord文件中的样例\n",
    "reader = tf.TFRecordReader()\n",
    "filename = tf.train.string_input_producer(['./data/tfrecord/test.tfrecords'])\n",
    "# filename = tf.train.string_input_producer(['./data/tfrecord/validation.tfrecords'])\n",
    "\n",
    "#从文件中读取一个样例\n",
    "_,one_example = reader.read(filename)\n",
    "#解析读入的数据\n",
    "features = tf.parse_single_example(one_example,features={\n",
    "    'labels':tf.FixedLenFeature([],tf.int64),\n",
    "    'image_raw':tf.FixedLenFeature([],tf.string),\n",
    "    'img_width':tf.FixedLenFeature([],tf.int64),\n",
    "    'img_high':tf.FixedLenFeature([],tf.int64)\n",
    "})\n",
    "#将字符串解析成图像对应的像素数组\n",
    "images = tf.decode_raw(features['image_raw'],tf.uint8)\n",
    "images = tf.reshape(images, [48, 64, 1])\n",
    "labels = tf.cast(features['labels'],tf.int32)\n",
    "width = tf.cast(features['img_width'],tf.int32)\n",
    "high = tf.cast(features['img_high'],tf.int32)\n",
    "\n",
    "images_bath, labels_bath = tf.train.shuffle_batch([images, labels], batch_size = 128, capacity = 4000, \n",
    "                                                  min_after_dequeue = 3888, \n",
    "                                                  num_threads = 2,\n",
    "                                                  shapes = ([48,64,1],[]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./cnn_txt/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./cnn_txt/model.ckpt\n",
      "epoch: 0 ,accuracy is: 0.753906 loss is : 0.85972\n",
      "epoch: 10 ,accuracy is: 0.751953 loss is : 0.739567\n",
      "epoch: 20 ,accuracy is: 0.818359 loss is : 0.599244\n",
      "epoch: 30 ,accuracy is: 0.792969 loss is : 0.60903\n",
      "epoch: 40 ,accuracy is: 0.822266 loss is : 0.559406\n",
      "epoch: 50 ,accuracy is: 0.837891 loss is : 0.473065\n",
      "epoch: 60 ,accuracy is: 0.888672 loss is : 0.351295\n",
      "epoch: 70 ,accuracy is: 0.904297 loss is : 0.3262\n",
      "epoch: 80 ,accuracy is: 0.921875 loss is : 0.230892\n",
      "epoch: 90 ,accuracy is: 0.943359 loss is : 0.207615\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "def hot(x):\n",
    "    b =[]\n",
    "    y = np.zeros([x.shape[0],4,11])\n",
    "    for i in range(x.shape[0]):\n",
    "        c= []\n",
    "        b.append(str(x[i]))\n",
    "        for j in range(len(b[i])):\n",
    "            c.append(int(b[i][j]))\n",
    "        while len(c)<4:\n",
    "            c.append(10)\n",
    "        for k in range(4):\n",
    "            y[i][k][c[k]] = 1\n",
    "    return y\n",
    "X = tf.placeholder(tf.float32, [None, 48, 64, 1])\n",
    "Y = tf.placeholder(tf.float32, [None, 4, 11])\n",
    "# images=tf.image.convert_image_dtype(image,tf.float32) \n",
    "# X = images\n",
    "# Y = hot(label,100)\n",
    "with tf.variable_scope('layer1'):\n",
    "    W1 = tf.Variable(tf.truncated_normal([5, 5, 1, 16], stddev = 0.1)) \n",
    "    b1 = tf.Variable(tf.constant(0.0, shape=[16]))\n",
    "    conv1 = tf.nn.conv2d(X, W1, strides = [1,1,1,1], padding = 'SAME')\n",
    "    Y0 = tf.nn.relu(tf.nn.bias_add(conv1, b1))\n",
    "    pool0 = tf.nn.max_pool(Y0,ksize=[1,2,2,1], strides=[1,2,2,1],padding = 'SAME')\n",
    "    \n",
    "    \n",
    "with tf.variable_scope('layer2'):\n",
    "    W2 = tf.Variable(tf.truncated_normal([5,5,16,32], stddev = 0.1))\n",
    "    b2 = tf.Variable(tf.constant(0.0, shape=[32]))\n",
    "    conv2 = tf.nn.conv2d(Y0, W2, strides = [1,1,1,1], padding = 'SAME') \n",
    "    Y1 = tf.nn.relu(tf.nn.bias_add(conv2, b2))\n",
    "    pool1 = tf.nn.max_pool(Y1,ksize=[1,2,2,1], strides=[1,2,2,1],padding = 'SAME')\n",
    "    \n",
    "    \n",
    "with tf.variable_scope('layer3'):\n",
    "    W3 = tf.Variable(tf.truncated_normal([3, 5, 32, 48], stddev = 0.1))\n",
    "    b3 = tf.Variable(tf.constant(0.0, shape = [48]))\n",
    "    conv3 = tf.nn.conv2d(pool1, W3, strides = [1,1,1,1], padding = 'SAME')\n",
    "    Y2 = tf.nn.relu(tf.nn.bias_add(conv3, b3))\n",
    "    pool2 = tf.nn.max_pool(Y2,ksize=[1,2,2,1], strides=[1,2,2,1],padding = 'SAME')\n",
    "    reshape = tf.reshape(pool2,[-1,12*16*48])\n",
    "#     pool_size = tf.shape(pool2)\n",
    "#     reshape = tf.reshape(pool2,[pool_size[0],-1])\n",
    "with tf.variable_scope('layer4'):\n",
    "    fc1_weight = tf.Variable(tf.truncated_normal([12*16*48,512],stddev = 0.1))\n",
    "    fc1_bias=tf.Variable(tf.constant(0.0,shape=[512]))\n",
    "    fc1 = tf.nn.relu(tf.matmul(reshape,fc1_weight)+fc1_bias)\n",
    "    \n",
    "    \n",
    "with tf.variable_scope('layer5'):\n",
    "    fc2_weight = tf.Variable(tf.truncated_normal([512,44], stddev = 0.1))\n",
    "    fc2_bias=tf.Variable(tf.constant(0.0,shape=[44]))\n",
    "    fc2 = tf.matmul(fc1,fc2_weight)+fc2_bias\n",
    "    fc21 = tf.reshape(fc2,[-1,4,11])\n",
    "    \n",
    "    \n",
    "with tf.variable_scope('cross_entropy'):\n",
    "    cross_entropy1 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y[:,0], logits=fc21[:,0]))\n",
    "    cross_entropy2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y[:,1], logits=fc21[:,1]))\n",
    "    cross_entropy3 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y[:,2], logits=fc21[:,2]))\n",
    "    cross_entropy4 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y[:,3], logits=fc21[:,3]))\n",
    "    cross_entropy = (cross_entropy1+cross_entropy2+cross_entropy3+cross_entropy4) / 4.\n",
    "# is_correct1 = tf.equal(tf.argmax(fc21[:,0]), tf.argmax(Y[:,0]))\n",
    "# is_correct2 = tf.equal(tf.argmax(fc21[:,1]), tf.argmax(Y[:,1]))\n",
    "# is_correct3 = tf.equal(tf.argmax(fc21[:,2]), tf.argmax(Y[:,2]))\n",
    "# is_correct4 = tf.equal(tf.argmax(fc21[:,3]), tf.argmax(Y[:,3]))\n",
    "is_correct = tf.equal(tf.argmax(fc21,2), tf.argmax(Y,2))\n",
    "# accuracy1 = tf.reduce_mean(tf.cast(is_correct1, tf.float32))\n",
    "# accuracy2 = tf.reduce_mean(tf.cast(is_correct2, tf.float32))\n",
    "# accuracy3 = tf.reduce_mean(tf.cast(is_correct3, tf.float32))\n",
    "# accuracy4 = tf.reduce_mean(tf.cast(is_correct4, tf.float32))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "# train_step1 = optimizer.minimize(cross_entropy1)\n",
    "# train_step2 = optimizer.minimize(cross_entropy2)\n",
    "# train_step3 = optimizer.minimize(cross_entropy3)\n",
    "# train_step4 = optimizer.minimize(cross_entropy4)\n",
    "train_step = optimizer.minimize(cross_entropy)\n",
    "# saver\n",
    "cnn_txt = \"./cnn_txt\"\n",
    "if not os.path.exists(cnn_txt):\n",
    "    os.makedirs(cnn_txt)\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    #启动多线程\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord = coord)\n",
    "    txt = tf.train.get_checkpoint_state(cnn_txt)\n",
    "    if txt and txt.model_checkpoint_path:\n",
    "        print(txt.model_checkpoint_path)\n",
    "        saver.restore(sess, txt.model_checkpoint_path)\n",
    "    for epoch in range(100):\n",
    "        # Load the input data\n",
    "        image, label = sess.run([images_bath, labels_bath])\n",
    "        train_data = {X: image / 255, Y: hot(label)}\n",
    "        # Train\n",
    "        sess.run([train_step],feed_dict = train_data)\n",
    "#         sess.run([ train_step1, train_step2, train_step3, train_step4], feed_dict = train_data)\n",
    "        # Accuracy on training data\n",
    "#         acc1, acc2, acc3, acc4 = sess.run([accuracy1, accuracy2, accuracy3, accuracy4], feed_dict = train_data)\n",
    "        acc, loss = sess.run([accuracy, cross_entropy],feed_dict = train_data)\n",
    "#         saver.save(sess,cnn_txt + \"/model.ckpt\")\n",
    "        if epoch % 10 ==0:\n",
    "            print(\"epoch: %d ,accuracy is: %g loss is : %g\" % (epoch, acc, loss))\n",
    "#             print(\"acc1: %g, acc2: %g, acc3: %g, acc4: %g\" % (acc1, acc2, acc3, acc4))\n",
    "#             print(\"loss1: %g, loss2: %g, loss3: %g loss4: %g\" % (loss1,loss2,loss3,loss4))\n",
    "#     print(\"acc_finally: %g\" % acc_fin)\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
