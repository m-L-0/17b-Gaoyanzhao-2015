{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "reader = tf.TFRecordReader()\n",
    "filename1 = tf.train.string_input_producer(['./data/tfrecord/train1.tfrecords'])\n",
    "filename2 = tf.train.string_input_producer(['./data/tfrecord/train2.tfrecords'])\n",
    "filename3 = tf.train.string_input_producer(['./data/tfrecord/train3.tfrecords'])\n",
    "filename4 = tf.train.string_input_producer(['./data/tfrecord/train4.tfrecords'])\n",
    "filename5 = tf.train.string_input_producer(['./data/tfrecord/train5.tfrecords'])\n",
    "filename6 = tf.train.string_input_producer(['./data/tfrecord/train6.tfrecords'])\n",
    "filename7 = tf.train.string_input_producer(['./data/tfrecord/train7.tfrecords'])\n",
    "filename8 = tf.train.string_input_producer(['./data/tfrecord/train8.tfrecords'])\n",
    "#创建一个reader来读取TFRecord文件中的样例\n",
    "def readtf(name):\n",
    "    filename = name\n",
    "    #从文件中读取一个样例\n",
    "    _,one_example = reader.read(filename)\n",
    "    #解析读入的数据\n",
    "    features = tf.parse_single_example(one_example,features={\n",
    "        'labels':tf.FixedLenFeature([],tf.int64),\n",
    "        'image_raw':tf.FixedLenFeature([],tf.string),\n",
    "        'img_width':tf.FixedLenFeature([],tf.int64),\n",
    "        'img_high':tf.FixedLenFeature([],tf.int64)\n",
    "    })\n",
    "    #将字符串解析成图像对应的像素数组\n",
    "    images = tf.decode_raw(features['image_raw'],tf.uint8)\n",
    "    images = tf.reshape(images, [48, 64, 1])\n",
    "    labels = tf.cast(features['labels'],tf.int32)\n",
    "    width = tf.cast(features['img_width'],tf.int32)\n",
    "    high = tf.cast(features['img_high'],tf.int32)\n",
    "    return images,labels\n",
    "x =[filename1, filename2, filename3, filename4, filename5, filename6, filename7, filename8] \n",
    "for i in x:\n",
    "    images,labels = readtf(i)\n",
    "images_bath, labels_bath = tf.train.shuffle_batch([images, labels], batch_size = 128, capacity = 4000, \n",
    "                                                  min_after_dequeue = 3888, \n",
    "                                                  num_threads = 2,\n",
    "                                                  shapes = ([48,64,1],[]))\n",
    "# def hot(x):\n",
    "#     b =[]\n",
    "#     y = np.zeros([x.shape[0],4,11])\n",
    "#     for i in range(x.shape[0]):\n",
    "#         c= []\n",
    "#         b.append(str(x[i]))\n",
    "#         for j in range(len(b[i])):\n",
    "#             c.append(int(b[i][j]))\n",
    "#         while len(c)<4:\n",
    "#             c.append(10)\n",
    "#         for k in range(4):\n",
    "#             y[i][k][c[k]] = 1\n",
    "#     return y\n",
    "# with tf.Session() as sess:\n",
    "#     coord = tf.train.Coordinator()\n",
    "#     threads = tf.train.start_queue_runners(sess=sess, coord = coord)\n",
    "#     image, label = sess.run([images_bath, labels_bath])\n",
    "#     Y = hot(label)\n",
    "#     print(np.argmax(Y,2))\n",
    "#     print(Y)\n",
    "#     coord.request_stop()\n",
    "#     coord.join(threads)    \n",
    "# with tf.Session() as sess:\n",
    "#     #启动多线程\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     sess.run(tf.local_variables_initializer())\n",
    "#     coord = tf.train.Coordinator()\n",
    "#     threads = tf.train.start_queue_runners(sess=sess, coord = coord)\n",
    "#     for i in range(4000):\n",
    "#         image, label = sess.run([images_bath, labels_bath])\n",
    "#         print(image.shape)\n",
    "#         print(label.shape)\n",
    "#         print(highs, widths)\n",
    "#         for i in range(1,100):\n",
    "#             plt.imshow(image[i].reshape(48, 64), cmap = 'gray')\n",
    "#             plt.show()\n",
    "#     coord.request_stop()\n",
    "#     coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 ,accuracy is: 0.318359 loss is : 11.5646\n",
      "epoch: 20 ,accuracy is: 0.388672 loss is : 1.92069\n",
      "epoch: 40 ,accuracy is: 0.394531 loss is : 1.873\n",
      "epoch: 60 ,accuracy is: 0.392578 loss is : 1.84983\n",
      "epoch: 80 ,accuracy is: 0.394531 loss is : 1.80327\n",
      "epoch: 100 ,accuracy is: 0.443359 loss is : 1.62228\n",
      "epoch: 120 ,accuracy is: 0.427734 loss is : 1.67012\n",
      "epoch: 140 ,accuracy is: 0.482422 loss is : 1.54373\n",
      "epoch: 160 ,accuracy is: 0.490234 loss is : 1.46211\n",
      "epoch: 180 ,accuracy is: 0.519531 loss is : 1.39037\n",
      "epoch: 200 ,accuracy is: 0.515625 loss is : 1.40331\n",
      "epoch: 220 ,accuracy is: 0.582031 loss is : 1.25718\n",
      "epoch: 240 ,accuracy is: 0.583984 loss is : 1.19924\n",
      "epoch: 260 ,accuracy is: 0.597656 loss is : 1.23562\n",
      "epoch: 280 ,accuracy is: 0.626953 loss is : 1.10994\n",
      "epoch: 300 ,accuracy is: 0.630859 loss is : 1.03874\n",
      "epoch: 320 ,accuracy is: 0.648438 loss is : 0.97091\n",
      "epoch: 340 ,accuracy is: 0.640625 loss is : 1.03425\n",
      "epoch: 360 ,accuracy is: 0.677734 loss is : 0.938929\n",
      "epoch: 380 ,accuracy is: 0.65625 loss is : 1.01525\n",
      "epoch: 400 ,accuracy is: 0.716797 loss is : 0.809417\n",
      "epoch: 420 ,accuracy is: 0.765625 loss is : 0.656184\n",
      "epoch: 440 ,accuracy is: 0.777344 loss is : 0.684849\n",
      "epoch: 460 ,accuracy is: 0.785156 loss is : 0.696248\n",
      "epoch: 480 ,accuracy is: 0.787109 loss is : 0.6054\n",
      "epoch: 500 ,accuracy is: 0.802734 loss is : 0.576382\n",
      "epoch: 520 ,accuracy is: 0.804688 loss is : 0.545411\n",
      "epoch: 540 ,accuracy is: 0.833984 loss is : 0.519421\n",
      "epoch: 560 ,accuracy is: 0.839844 loss is : 0.483952\n",
      "epoch: 580 ,accuracy is: 0.841797 loss is : 0.459758\n",
      "epoch: 600 ,accuracy is: 0.878906 loss is : 0.383054\n",
      "epoch: 620 ,accuracy is: 0.841797 loss is : 0.473853\n",
      "epoch: 640 ,accuracy is: 0.863281 loss is : 0.396673\n",
      "epoch: 660 ,accuracy is: 0.892578 loss is : 0.312041\n",
      "epoch: 680 ,accuracy is: 0.867188 loss is : 0.349383\n",
      "epoch: 700 ,accuracy is: 0.894531 loss is : 0.283504\n",
      "epoch: 720 ,accuracy is: 0.896484 loss is : 0.296697\n",
      "epoch: 740 ,accuracy is: 0.900391 loss is : 0.299966\n",
      "epoch: 760 ,accuracy is: 0.898438 loss is : 0.305126\n",
      "epoch: 780 ,accuracy is: 0.914062 loss is : 0.271216\n",
      "epoch: 800 ,accuracy is: 0.919922 loss is : 0.234356\n",
      "epoch: 820 ,accuracy is: 0.921875 loss is : 0.216711\n",
      "epoch: 840 ,accuracy is: 0.908203 loss is : 0.267794\n",
      "epoch: 860 ,accuracy is: 0.933594 loss is : 0.192834\n",
      "epoch: 880 ,accuracy is: 0.916016 loss is : 0.214899\n",
      "epoch: 900 ,accuracy is: 0.935547 loss is : 0.176384\n",
      "epoch: 920 ,accuracy is: 0.947266 loss is : 0.156866\n",
      "epoch: 940 ,accuracy is: 0.917969 loss is : 0.19361\n",
      "epoch: 960 ,accuracy is: 0.933594 loss is : 0.189079\n",
      "epoch: 980 ,accuracy is: 0.947266 loss is : 0.144948\n",
      "epoch: 1000 ,accuracy is: 0.949219 loss is : 0.165322\n",
      "epoch: 1020 ,accuracy is: 0.9375 loss is : 0.174891\n",
      "epoch: 1040 ,accuracy is: 0.947266 loss is : 0.182921\n",
      "epoch: 1060 ,accuracy is: 0.951172 loss is : 0.141767\n",
      "epoch: 1080 ,accuracy is: 0.949219 loss is : 0.162343\n",
      "epoch: 1100 ,accuracy is: 0.970703 loss is : 0.100375\n",
      "epoch: 1120 ,accuracy is: 0.945312 loss is : 0.128726\n",
      "epoch: 1140 ,accuracy is: 0.960938 loss is : 0.0974053\n",
      "epoch: 1160 ,accuracy is: 0.964844 loss is : 0.0972295\n",
      "epoch: 1180 ,accuracy is: 0.955078 loss is : 0.149777\n",
      "epoch: 1200 ,accuracy is: 0.953125 loss is : 0.121033\n",
      "epoch: 1220 ,accuracy is: 0.984375 loss is : 0.0756994\n",
      "epoch: 1240 ,accuracy is: 0.966797 loss is : 0.0913845\n",
      "epoch: 1260 ,accuracy is: 0.96875 loss is : 0.0932041\n",
      "epoch: 1280 ,accuracy is: 0.972656 loss is : 0.0739349\n",
      "epoch: 1300 ,accuracy is: 0.955078 loss is : 0.11268\n",
      "epoch: 1320 ,accuracy is: 0.943359 loss is : 0.170207\n",
      "epoch: 1340 ,accuracy is: 0.966797 loss is : 0.111288\n",
      "epoch: 1360 ,accuracy is: 0.964844 loss is : 0.0960712\n",
      "epoch: 1380 ,accuracy is: 0.978516 loss is : 0.076457\n",
      "epoch: 1400 ,accuracy is: 0.978516 loss is : 0.0908432\n",
      "epoch: 1420 ,accuracy is: 0.972656 loss is : 0.0668128\n",
      "epoch: 1440 ,accuracy is: 0.972656 loss is : 0.0826848\n",
      "epoch: 1460 ,accuracy is: 0.962891 loss is : 0.0897773\n",
      "epoch: 1480 ,accuracy is: 0.96875 loss is : 0.0871923\n",
      "epoch: 1500 ,accuracy is: 0.964844 loss is : 0.0880452\n",
      "epoch: 1520 ,accuracy is: 0.978516 loss is : 0.0595147\n",
      "epoch: 1540 ,accuracy is: 0.972656 loss is : 0.081662\n",
      "epoch: 1560 ,accuracy is: 0.972656 loss is : 0.0810618\n",
      "epoch: 1580 ,accuracy is: 0.982422 loss is : 0.0670383\n",
      "epoch: 1600 ,accuracy is: 0.984375 loss is : 0.0532305\n",
      "epoch: 1620 ,accuracy is: 0.962891 loss is : 0.11549\n",
      "epoch: 1640 ,accuracy is: 0.976562 loss is : 0.0668802\n",
      "epoch: 1660 ,accuracy is: 0.962891 loss is : 0.110442\n",
      "epoch: 1680 ,accuracy is: 0.966797 loss is : 0.0830298\n",
      "epoch: 1700 ,accuracy is: 0.978516 loss is : 0.0837632\n",
      "epoch: 1720 ,accuracy is: 0.974609 loss is : 0.0557104\n",
      "epoch: 1740 ,accuracy is: 0.972656 loss is : 0.0684447\n",
      "epoch: 1760 ,accuracy is: 0.986328 loss is : 0.0610538\n",
      "epoch: 1780 ,accuracy is: 0.974609 loss is : 0.0746943\n",
      "epoch: 1800 ,accuracy is: 0.976562 loss is : 0.0768757\n",
      "epoch: 1820 ,accuracy is: 0.96875 loss is : 0.0918189\n",
      "epoch: 1840 ,accuracy is: 0.974609 loss is : 0.0811857\n",
      "epoch: 1860 ,accuracy is: 0.964844 loss is : 0.0870271\n",
      "epoch: 1880 ,accuracy is: 0.976562 loss is : 0.0934042\n",
      "epoch: 1900 ,accuracy is: 0.984375 loss is : 0.0470457\n",
      "epoch: 1920 ,accuracy is: 0.980469 loss is : 0.0532179\n",
      "epoch: 1940 ,accuracy is: 0.978516 loss is : 0.058338\n",
      "epoch: 1960 ,accuracy is: 0.980469 loss is : 0.044453\n",
      "epoch: 1980 ,accuracy is: 0.978516 loss is : 0.0468398\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "def hot(x):\n",
    "    b =[]\n",
    "    y = np.zeros([x.shape[0],4,11])\n",
    "    for i in range(x.shape[0]):\n",
    "        c= []\n",
    "        b.append(str(x[i]))\n",
    "        for j in range(len(b[i])):\n",
    "            c.append(int(b[i][j]))\n",
    "        while len(c)<4:\n",
    "            c.append(10)\n",
    "        for k in range(4):\n",
    "            y[i][k][c[k]] = 1\n",
    "    return y\n",
    "X = tf.placeholder(tf.float32, [None, 48, 64, 1])\n",
    "Y = tf.placeholder(tf.float32, [None, 4, 11])\n",
    "# images=tf.image.convert_image_dtype(image,tf.float32) \n",
    "# X = images\n",
    "# Y = hot(label,100)\n",
    "with tf.variable_scope('layer1'):\n",
    "    W1 = tf.Variable(tf.truncated_normal([5, 5, 1, 16], stddev = 0.1)) \n",
    "    b1 = tf.Variable(tf.constant(0.0, shape=[16]))\n",
    "    conv1 = tf.nn.conv2d(X, W1, strides = [1,1,1,1], padding = 'SAME')\n",
    "    Y0 = tf.nn.relu(tf.nn.bias_add(conv1, b1))\n",
    "    pool0 = tf.nn.max_pool(Y0,ksize=[1,2,2,1], strides=[1,2,2,1],padding = 'SAME')\n",
    "    \n",
    "    \n",
    "with tf.variable_scope('layer2'):\n",
    "    W2 = tf.Variable(tf.truncated_normal([5,5,16,32], stddev = 0.1))\n",
    "    b2 = tf.Variable(tf.constant(0.0, shape=[32]))\n",
    "    conv2 = tf.nn.conv2d(Y0, W2, strides = [1,1,1,1], padding = 'SAME') \n",
    "    Y1 = tf.nn.relu(tf.nn.bias_add(conv2, b2))\n",
    "    pool1 = tf.nn.max_pool(Y1,ksize=[1,2,2,1], strides=[1,2,2,1],padding = 'SAME')\n",
    "    \n",
    "    \n",
    "with tf.variable_scope('layer3'):\n",
    "    W3 = tf.Variable(tf.truncated_normal([3, 5, 32, 48], stddev = 0.1))\n",
    "    b3 = tf.Variable(tf.constant(0.0, shape = [48]))\n",
    "    conv3 = tf.nn.conv2d(pool1, W3, strides = [1,1,1,1], padding = 'SAME')\n",
    "    Y2 = tf.nn.relu(tf.nn.bias_add(conv3, b3))\n",
    "    pool2 = tf.nn.max_pool(Y2,ksize=[1,2,2,1], strides=[1,2,2,1],padding = 'SAME')\n",
    "    reshape = tf.reshape(pool2,[-1,12*16*48])\n",
    "#     pool_size = tf.shape(pool2)\n",
    "#     reshape = tf.reshape(pool2,[pool_size[0],-1])\n",
    "with tf.variable_scope('layer4'):\n",
    "    fc1_weight = tf.Variable(tf.truncated_normal([12*16*48,512],stddev = 0.1))\n",
    "    fc1_bias=tf.Variable(tf.constant(0.0,shape=[512]))\n",
    "    fc1 = tf.nn.relu(tf.matmul(reshape,fc1_weight)+fc1_bias)\n",
    "    fc1_drop = tf.nn.dropout(fc1, 0.5)\n",
    "    \n",
    "with tf.variable_scope('layer5'):\n",
    "    fc2_weight = tf.Variable(tf.truncated_normal([512,44], stddev = 0.1))\n",
    "    fc2_bias=tf.Variable(tf.constant(0.0,shape=[44]))\n",
    "    fc2 = tf.matmul(fc1_drop,fc2_weight)+fc2_bias\n",
    "    fc21 = tf.reshape(fc2,[-1,4,11])\n",
    "    \n",
    "    \n",
    "with tf.variable_scope('cross_entropy'):\n",
    "    cross_entropy1 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y[:,0], logits=fc21[:,0]))\n",
    "    cross_entropy2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y[:,1], logits=fc21[:,1]))\n",
    "    cross_entropy3 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y[:,2], logits=fc21[:,2]))\n",
    "    cross_entropy4 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y[:,3], logits=fc21[:,3]))\n",
    "    cross_entropy = (cross_entropy1+cross_entropy2+cross_entropy3+cross_entropy4) / 4.\n",
    "# is_correct1 = tf.equal(tf.argmax(fc21[:,0]), tf.argmax(Y[:,0]))\n",
    "# is_correct2 = tf.equal(tf.argmax(fc21[:,1]), tf.argmax(Y[:,1]))\n",
    "# is_correct3 = tf.equal(tf.argmax(fc21[:,2]), tf.argmax(Y[:,2]))\n",
    "# is_correct4 = tf.equal(tf.argmax(fc21[:,3]), tf.argmax(Y[:,3]))\n",
    "is_correct = tf.equal(tf.argmax(fc21,2), tf.argmax(Y,2))\n",
    "# accuracy1 = tf.reduce_mean(tf.cast(is_correct1, tf.float32))\n",
    "# accuracy2 = tf.reduce_mean(tf.cast(is_correct2, tf.float32))\n",
    "# accuracy3 = tf.reduce_mean(tf.cast(is_correct3, tf.float32))\n",
    "# accuracy4 = tf.reduce_mean(tf.cast(is_correct4, tf.float32))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "# train_step1 = optimizer.minimize(cross_entropy1)\n",
    "# train_step2 = optimizer.minimize(cross_entropy2)\n",
    "# train_step3 = optimizer.minimize(cross_entropy3)\n",
    "# train_step4 = optimizer.minimize(cross_entropy4)\n",
    "train_step = optimizer.minimize(cross_entropy)\n",
    "# saver\n",
    "cnn_txt = \"./cnn_txt\"\n",
    "if not os.path.exists(cnn_txt):\n",
    "    os.makedirs(cnn_txt)\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    #启动多线程\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord = coord)\n",
    "    txt = tf.train.get_checkpoint_state(cnn_txt)\n",
    "    if txt and txt.model_checkpoint_path:\n",
    "        print(txt.model_checkpoint_path)\n",
    "        saver.restore(sess, txt.model_checkpoint_path)\n",
    "    for epoch in range(2000):\n",
    "        # Load the input data\n",
    "        image, label = sess.run([images_bath, labels_bath])\n",
    "        train_data = {X: image / 255., Y: hot(label)}\n",
    "        # Train\n",
    "        sess.run([train_step],feed_dict = train_data)\n",
    "#         sess.run([ train_step1, train_step2, train_step3, train_step4], feed_dict = train_data)\n",
    "        # Accuracy on training data\n",
    "#         acc1, acc2, acc3, acc4 = sess.run([accuracy1, accuracy2, accuracy3, accuracy4], feed_dict = train_data)\n",
    "        acc, loss = sess.run([accuracy, cross_entropy],feed_dict = train_data)\n",
    "        saver.save(sess,cnn_txt + \"/model.ckpt\")\n",
    "        if epoch % 20 ==0:\n",
    "            print(\"epoch: %d ,accuracy is: %g loss is : %g\" % (epoch, acc, loss))\n",
    "#             print(\"acc1: %g, acc2: %g, acc3: %g, acc4: %g\" % (acc1, acc2, acc3, acc4))\n",
    "#             print(\"loss1: %g, loss2: %g, loss3: %g loss4: %g\" % (loss1,loss2,loss3,loss4))\n",
    "#     print(\"acc_finally: %g\" % acc_fin)\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
